{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from torchmetrics import AUROC\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchfm.dataset.criteo import CriteoDataset\n",
    "from torchfm.dataset.movielens import MovieLens1MDataset, MovieLens20MDataset\n",
    "\n",
    "from torchfm.model.dcn import DeepCrossNetworkModel\n",
    "from torchfm.model.dfm import DeepFactorizationMachineModel\n",
    "from torchfm.model.fm import FactorizationMachineModel\n",
    "from torchfm.model.wd import WideAndDeepModel\n",
    "from torchfm.model.afm import AttentionalFactorizationMachineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure device, batch_size and metric.\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = 512\n",
    "auroc = AUROC(task=\"binary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `torchfm` to implement model and dataset interfaces.\n",
    "- Make sure to download `Movielens1M`, `Movielens20M`, `Criteo` datasets and put in the correct paths based on `dataset_paths`.\n",
    "- For the first time running this notebook please train the DRS models. For example, you can use the following bash command to train `DCN` with `MovieLens-1M` dataset:\n",
    "    ```bash\n",
    "    python3 ./train.py --dataset_name movielens1M --dataset_path ./ml-1m/ratings.dat --model_name dcn --save_dir ./chkpt\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name, path):\n",
    "    if name == 'movielens1M':\n",
    "        return MovieLens1MDataset(path)\n",
    "    elif name == 'movielens20M':\n",
    "        return MovieLens20MDataset(path)\n",
    "    elif name == 'criteo':\n",
    "        return CriteoDataset(path)\n",
    "    else:\n",
    "        raise ValueError('unknown dataset name: ' + name)\n",
    "\n",
    "def get_model(name, dataset):\n",
    "    field_dims = dataset.field_dims\n",
    "    if name == 'fm':\n",
    "        return FactorizationMachineModel(field_dims, embed_dim=16)\n",
    "    elif name == 'wd':\n",
    "        return WideAndDeepModel(field_dims, embed_dim=16, mlp_dims=(16, 16), dropout=0.2)\n",
    "    elif name == 'dcn':\n",
    "        return DeepCrossNetworkModel(field_dims, embed_dim=16, num_layers=3, mlp_dims=(16, 16), dropout=0.2)\n",
    "    elif name == 'dfm':\n",
    "        return DeepFactorizationMachineModel(field_dims, embed_dim=16, mlp_dims=(16, 16), dropout=0.2)\n",
    "    elif name == 'afm':\n",
    "        return AttentionalFactorizationMachineModel(field_dims, embed_dim=16, LNN_dim=1500, mlp_dims=(400, 400, 400), dropouts=(0, 0, 0))\n",
    "    else:\n",
    "        raise ValueError('unknown model name: ' + name)\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    targets, predicts = [], []\n",
    "    with torch.no_grad():\n",
    "        for fields, target in tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0):\n",
    "            fields, target = fields.to(device), target.to(device)\n",
    "            y = model(fields)\n",
    "            targets.extend(target.cpu())\n",
    "            predicts.extend(y.cpu())\n",
    "        targets = torch.FloatTensor(targets).squeeze()\n",
    "        predicts = torch.FloatTensor(predicts).squeeze()\n",
    "    return auroc(predicts, targets)\n",
    "\n",
    "def testset_prepare(dataset_name, dataset_path):\n",
    "    dataset = get_dataset(dataset_name, dataset_path)\n",
    "    train_length = int(len(dataset) * 0.8)\n",
    "    valid_length = int(len(dataset) * 0.1)\n",
    "    test_length = len(dataset) - train_length - valid_length\n",
    "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_length, valid_length, test_length))\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    return test_data_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Injection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sweeping datasets, models and BERs. \n",
    "- Modify `param_names = ['mlp', 'fc', 'afm']` to change targets for error injection / protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from terrorch.terrorch import Injector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './chkpt/'\n",
    "dataset_names = ['movielens1M', 'movielens20M', 'criteo']\n",
    "dataset_paths = ['./ml-1m/ratings.dat', './MovieLens20M/rating.csv', './criteo-dac/train.txt']\n",
    "model_paths = ['fm', 'dcn', 'afm', 'wd', 'dfm']\n",
    "datasets = ['_movielens1M.pt', '_movielens20M.pt', '_criteo.pt']\n",
    "bers = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, ]\n",
    "bers = bers[::-1]\n",
    "folds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ]\n",
    "\n",
    "results = torch.zeros(size = (len(model_paths), len(datasets), len(bers), len(folds)))\n",
    "\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for dataset_i, dataset in enumerate(datasets):\n",
    "        test_data_loader = testset_prepare(dataset_names[dataset_i], dataset_paths[dataset_i])\n",
    "        for model_path_i, model_path in enumerate(model_paths):\n",
    "            for ber_i, ber in enumerate(bers):\n",
    "                for fold_i, fold in enumerate(folds):\n",
    "                    model = torch.load(base_dir + model_path + dataset)\n",
    "                    model = model.float().eval().to(device)\n",
    "                    injector = Injector(ber, param_names = ['mlp', 'fc', 'afm'], device = device, verbose = False)\n",
    "                    injector.inject(model)\n",
    "                    del injector\n",
    "                    result = test(model, test_data_loader, device)\n",
    "                    print(cnt, result)\n",
    "                    results[model_path_i][dataset_i][ber_i][fold_i] = result\n",
    "                    cnt += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To implement custom mitigation. please implement as `@classmethod` in `deterrorch.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './chkpt/'\n",
    "dataset_names = ['movielens1M', 'movielens20M', 'criteo']\n",
    "dataset_paths = ['./ml-1m/ratings.dat', './MovieLens20M/rating.csv', './criteo-dac/train.txt']\n",
    "model_paths = ['fm', 'dcn', 'afm', 'wd', 'dfm']\n",
    "datasets = ['_movielens1M.pt', '_movielens20M.pt', '_criteo.pt']\n",
    "bers = [1e-2, 1e-3, 1e-4, 1e-5,]\n",
    "bers = bers[::-1]\n",
    "folds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ]\n",
    "\n",
    "results = torch.zeros(size = (len(model_paths), len(datasets), len(bers), len(folds)))\n",
    "\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for dataset_i, dataset in enumerate(datasets):\n",
    "        test_data_loader = testset_prepare(dataset_names[dataset_i], dataset_paths[dataset_i])\n",
    "        for model_path_i, model_path in enumerate(model_paths):\n",
    "            for ber_i, ber in enumerate(bers):\n",
    "                for fold_i, fold in enumerate(folds):\n",
    "                    model = torch.load(base_dir + model_path + dataset)\n",
    "                    model = model.float().eval().to(device)\n",
    "                    injector = Injector(ber, param_names = ['mlp', 'fc', 'afm'], device = device, verbose = False, mitigation = 'clip')\n",
    "                    injector.inject(model)\n",
    "                    injector.perform_mitigation(model)\n",
    "                    del injector\n",
    "                    result = test(model, test_data_loader, device)\n",
    "                    print(cnt, result)\n",
    "                    results[model_path_i][dataset_i][ber_i][fold_i] = result\n",
    "                    cnt += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selective Bit Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './chkpt/'\n",
    "dataset_names = ['movielens1M', 'movielens20M', 'criteo']\n",
    "dataset_paths = ['./ml-1m/ratings.dat', './MovieLens20M/rating.csv', './criteo-dac/train.txt']\n",
    "model_paths = ['fm', 'dcn', 'afm', 'wd', 'dfm']\n",
    "datasets = ['_movielens1M.pt', '_movielens20M.pt', '_criteo.pt']\n",
    "bers = [1e-2, 1e-3, 1e-4, 1e-5,]\n",
    "bers = bers[::-1]\n",
    "folds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ]\n",
    "\n",
    "results = torch.zeros(size = (len(model_paths), len(datasets), len(bers), len(folds)))\n",
    "\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for dataset_i, dataset in enumerate(datasets):\n",
    "        test_data_loader = testset_prepare(dataset_names[dataset_i], dataset_paths[dataset_i])\n",
    "        for model_path_i, model_path in enumerate(model_paths):\n",
    "            for ber_i, ber in enumerate(bers):\n",
    "                for fold_i, fold in enumerate(folds):\n",
    "                    model = torch.load(base_dir + model_path + dataset)\n",
    "                    model = model.float().eval().to(device)\n",
    "                    injector = Injector(ber, param_names = ['mlp', 'fc', 'afm'], device = device, verbose = False, mitigation = 'SBP')\n",
    "                    injector.perform_mitigation(injector)\n",
    "                    injector.inject(model)\n",
    "                    del injector\n",
    "                    result = test(model, test_data_loader, device)\n",
    "                    print(cnt, result)\n",
    "                    results[model_path_i][dataset_i][ber_i][fold_i] = result\n",
    "                    cnt += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
