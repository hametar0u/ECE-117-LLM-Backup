{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72dc17a-93e0-42ea-9e2d-95696b6ac60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# this prints out the named parameters of a model\n",
    "def print_named_params(model: nn.Module) -> None:\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "\n",
    "def output_targets(model: nn.Module, file: str) -> None:\n",
    "    with open(file, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9042c-0d93-4d91-b02e-fecd5320f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "#import timm\n",
    "torch.set_printoptions(precision = 6, sci_mode = False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36799eac-0990-4aeb-9d8b-083b2ae5e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../pytei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b02be-e027-46ca-b1a5-98c884dc5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fb192-a436-4a71-97bf-b5ec8e3db63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytei import Injector\n",
    "def inject_error(model: nn.Module, error_map_file: str, prob) -> nn.Module:\n",
    "    model_error = deepcopy(model).to(device)\n",
    "    injector = Injector(error_map_file, p = prob, device = device, verbose = True)\n",
    "    injector.inject(model_error)\n",
    "    return model_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd5871-9df3-490f-a87c-68ebc4821cd3",
   "metadata": {},
   "source": [
    "## GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69d043-19ce-44ab-aadd-162ed60b5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6235a7b-b8db-47fc-862f-03128de51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2-medium')\n",
    "\n",
    "def get_modified_state_dictGPT2(model: nn.Module):\n",
    "    new_state_dict = deepcopy(model.state_dict())\n",
    "    for key in list(new_state_dict.keys()):\n",
    "        new_state_dict[f\"transformer.{key}\"] = new_state_dict.pop(key)\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124bacd-3227-45d9-96fc-12fb31c9892c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_named_params(gpt2)\n",
    "output_targets(gpt2, \"gpt2_targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb3fab-681e-4ba1-afea-cf0b589e58bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt2_error = inject_error(gpt2, \"gpt2_targets\")\n",
    "gpt2_error.eval()\n",
    "text = \"blahblahblah\"\n",
    "test_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    error_out = gpt2_error(**test_input) # gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35275cb2-b50e-41e9-997f-73bdecf03dca",
   "metadata": {},
   "source": [
    "## MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa435f96-ede5-4424-b7bc-753763bcd349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8dbb43-33cb-4a97-8b63-eca53df09a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f761ed-99ee-464b-848f-9a7b6f4bf999",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dd06f-29ee-4135-a34a-d3837c566909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186500e8-665b-47f5-bcf2-8d2debf3aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,  AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "class GPT2(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        # Same as the previous example above\n",
    "        model = self.load_model()\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Create parser required for JSON confinement using lmformatenforcer\n",
    "        parser = JsonSchemaParser(schema.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
    "            pipeline.tokenizer, parser\n",
    "        )\n",
    "\n",
    "        # Output and load valid JSON\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt) :]\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        # Return valid JSON object according to the schema DeepEval supplied\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"GPT2\"\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\") # Can be replaced with any huggingface model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76848d50-077c-44d4-8951-925cf77f2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from deepeval.benchmarks import MMLU\n",
    "from deepeval.benchmarks.tasks import MMLUTask\n",
    "\n",
    "'''\n",
    "def convert_torch_to_huggingface(model: torch.nn.Module):\n",
    "    config = AutoConfig.from_pretrained(\"gpt2\") # Change to whichever model architecture being used\n",
    "    hf_model = AutoModelForCausalLM.from_config(config)    \n",
    "    hf_model.load_state_dict(model.state_dict(), strict=False)\n",
    "    return hf_model\n",
    "'''\n",
    "\n",
    "def convert_torch_to_huggingface_stateGPT2(state_dict):\n",
    "    config = AutoConfig.from_pretrained(\"gpt2-medium\") # Change to whichever model architecture being used\n",
    "    hf_model = AutoModelForCausalLM.from_config(config)    \n",
    "    hf_model.load_state_dict(state_dict, strict=False)\n",
    "    return hf_model\n",
    "\n",
    "def evaluate_model_MMLU(model):\n",
    "\n",
    "    benchmark = MMLU(\n",
    "        tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n",
    "        n_shots=3\n",
    "    )\n",
    "\n",
    "    benchmark.evaluate(model=model)\n",
    "    return benchmark.task_scores\n",
    "\n",
    "def convert_to_hf_GPT2(model: nn.Module):\n",
    "    return convert_torch_to_huggingface_stateGPT2(get_modified_state_dictGPT2(model))\n",
    "    \n",
    "def evaluate_model_MMLU_GPT2(model: nn.Module):\n",
    "    hf_model = convert_to_hf_GPT2(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    test_model = GPT2(model=hf_model, tokenizer=tokenizer)\n",
    "    return evaluate_model_MMLU(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652ffeb-8309-4ea5-b60e-d7545f7e60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "output_targets(model, \"gpt2_targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4252fb8-eff2-456f-862d-c90fa538e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_named_targets(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aec3bb-9b57-463b-8d16-1550deffada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_error(model, \"gpt2_targets\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1b237-17a8-429f-b92a-fd15f4f834ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        token=token,\n",
    "    )\n",
    "mistral = GPT2(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ceeaf6-34fe-4846-add0-ccead613e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = [2e-9, 4e-9, 6e-9, 8e-9]\n",
    "for prob in probability:\n",
    "    while True:\n",
    "        test_model = inject_error(gpt2, \"gpt2_targets\", prob)\n",
    "        try:\n",
    "            result = evaluate_model_MMLU_GPT2(test_model)\n",
    "            for i in result.index:\n",
    "                task = result.loc[i, \"Task\"]\n",
    "                score = result.loc[i, \"Score\"]\n",
    "                with open(f\"results/gpt2_{str(prob)}_{task}\", \"w\") as f:\n",
    "                    f.write(f\"{str(score)}\\n\") \n",
    "            print(\"Succeeded no nan\")\n",
    "            break\n",
    "        except:\n",
    "            print(\"Got an error\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d05f71-6f1e-44c3-97c6-160e87936bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = gpt2\n",
    "result = evaluate_model_MMLU_GPT2(test_model)\n",
    "for i in result.index:\n",
    "    task = result.loc[i, \"Task\"]\n",
    "    score = result.loc[i, \"Score\"]\n",
    "    with open(f\"results/gpt2_{str(0)}_{task}\", \"w\") as f:\n",
    "        f.write(f\"{str(score)}\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa011f-6cf8-4027-a502-c2ca6c1da3ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = evaluate_model_MMLU_GPT2(gpt2_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ceaaf-3f90-449d-b2f6-a5546d2d7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc94a3d-ffeb-4fb4-bff8-85b67f39d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[result[\"Task\"] == \"high_school_computer_science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ef8ab-926c-4c76-bcd0-dec163dd1261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce79a8d-836e-4e46-8847-82cc44ff461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456cf02-3359-4b62-a7b3-592d59103963",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result.index:\n",
    "    task = result.loc[i, \"Task\"]\n",
    "    score = result.loc[i, \"Score\"]\n",
    "    with open(f\"results/gpt2_{str(prob)}_{task}\", \"w\") as f:\n",
    "        f.write(f\"{str(score)}\\n\") \n",
    "    print(\"Succeeded no nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8431c90-3e9d-49a7-9bb6-bc9d966b7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ccc87-cf0f-4f79-a6f0-5716ecd690b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
