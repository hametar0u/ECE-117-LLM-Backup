{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../pytei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytei import Injector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 56.567766,
     "end_time": "2023-10-20T22:56:37.983903",
     "exception": false,
     "start_time": "2023-10-20T22:55:41.416137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U pydantic\n",
    "!pip install -q -U lm-format-enforcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 8.384116,
     "end_time": "2023-10-20T22:56:46.370893",
     "exception": false,
     "start_time": "2023-10-20T22:56:37.986777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.012477,
     "end_time": "2023-10-20T22:59:33.492407",
     "exception": false,
     "start_time": "2023-10-20T22:59:33.47993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,  AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from torch import nn\n",
    "class GPT2(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Create parser required for JSON confinement using lmformatenforcer\n",
    "        parser = JsonSchemaParser(schema.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
    "            pipeline.tokenizer, parser\n",
    "        )\n",
    "\n",
    "        # Output and load valid JSON\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt) :]\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        # Return valid JSON object according to the schema DeepEval supplied\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"GPT2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        token=token,\n",
    "    )\n",
    "mistral = GPT2(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def print_named_params(model: nn.Module) -> None:\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "def output_targets(model: nn.Module, file: str, regex: str) -> None:\n",
    "    with open(f'{file}', 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            if (re.match(regex, name)):\n",
    "                f.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    shape = param.shape\n",
    "    dtype = param.dtype\n",
    "    mask = torch.tensor((torch.rand(shape) < 0.01).long(), dtype=dtype)\n",
    "    op_mask = torch.ones(shape, dtype=dtype) - mask\n",
    "    rand_value = torch.tensor(torch.randint(torch.iinfo(torch.int64).max, shape)).view(dtype=dtype).split(shape[-1], dim=-1)[0].nan_to_num(0)\n",
    "    new_param = mask*rand_value + param.data*op_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/kw/q3njdyrn6fd9tgt5822syclw0000gn/T/ipykernel_32160/1494076513.py:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  output_targets(model, \"gpt2targets\", \"transformer\\.h\\.11.*\")\n"
     ]
    }
   ],
   "source": [
    "output_targets(model, \"gpt2targets\", \"transformer\\.h\\.11.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "device = \"cpu\"\n",
    "prob = 0.000001\n",
    "model_error = deepcopy(model).to(device)\n",
    "injector = Injector(\"gpt2targets\", p = prob, device = device)\n",
    "injector.inject_values(model_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import MMLU\n",
    "from deepeval.benchmarks.tasks import MMLUTask\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = MMLU(\n",
    "    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n",
    "    n_shots=2\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand((3, 5), dtype=torch.float16)\n",
    "A = torch.rand((3, 5), dtype=torch.float16).view(dtype=torch.uint8)[:3, :5]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (torch.rand((3, 5), dtype=torch.float) < 0.1).long()\n",
    "mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "opposite = torch.ones((3, 5), dtype=torch.float16)\n",
    "print(mask)\n",
    "print(opposite)\n",
    "opposite-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6157206,
     "sourceId": 10003104,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 1902,
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
