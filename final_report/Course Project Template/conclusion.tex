\section{Discussions}
There are still many possible things that one could try. As a group, we were heavily restricted by our computational resources, so with more computation resources
and more time it would be possible to try many more different things, as well as trying some more state of the art models with many more parameters than
Mistral-7B.

It could be interesting to try inject errors into specific sections of each layer (such as attention or normalization), instead of just all the layers and see
or all sections in a particular layer. We have confidence that this could give interesting results because of the high variation in some of our injection parameters,
which suggests that where our random errors actually are injected into even with our given parameters is also important.

It would also be interesting to look at the performance degradation on various other benchmarks such as translation, coding capabilities, or reasoning tasks.

Our research is also limited in the sense that we looked at value injections. As a whole, silent bit errors seem to be completely catastrophic for
some standard machine learning implementation. For example, in the usual case where the weights lie between $0$ and $1$, we've shown in \ref{sec:nan} that NaN
is likely for any large amount of bit errors. In float16 representation, the errors tend to cause overflow which broke the evaluation framework that we were working in.
The final case is int4 quantization. Here, bit errors actually tend to be fine. In addition, random bit errors here are close to random value errors, which justifies our choice to look at
random value errors instead.

\textit{Aside on RMSE:} We also tried to use RMSE between the output logits of the target and corrupted model as a metric for evaluation. However, each question in the MMLU benchmark is multiple-choice with 4 options and expects only a single new output token. With only 1 new token, RMSE is impractical as a metric for such a benchmark. However, it should be noted that RMSE would be more interesting with an open-ended question answering or text generation benchmark. This can give us greater insight into how the errors in each layer are affecting the model's inference capabilities since RMSE is more sensitive to deviations. Moreover, RMSE will also provide a more robust outlook on the effects of error injection since it is relatively benchmark independent.

\section{Conclusion}
\label{sec:conclusion}
The large takeaway is that all value errors are certainly not equal. It seems that errors in earlier levels tend to degrade the power of our LLM significantly more than errors at later levels.
In addition, it seems that certain parts of the LLM tend to be more resistant to errors (such as the batch-normalization layer). In addition, we have shown that, in general,
random bit errors in an LLM's weights tend to be catastrophic in the sense they have a high probability of completely breaking the LLM.

The limitation in our work is in connecting these two cases. It's unknown if our small value errors are a good model for bit errors that do not completely break the LLM. Hence, it's unknown whether
large variation errors will have a larger degrading effect than the ones we have injected. In addition, it is still unknown if our results on error degradation for specific layers still hold for
the larger case of completely random bit errors.

More research is needed for all the above mentioned cases to draw stronger conclusions on the security of LLM's to random bit flips.

\section{Contributions of each teammate}
\textit{Abhi} worked on the notebook for injecting errors by component, performed the runs for this experiment on Kaggle, and performed the analysis of the results. Abhi also modified the error injection library to output the number of injected parameters. Abhi wrote sections \textbf{4.2 Design Approaches} and \textbf{6.3 Variation by Component} of this report.

\textit{Allen} developed the notebook for running the experiments and ran runs of the experiment on Kaggle. He wrote sections \textbf{1 Introduction}, \textbf{5.3 Evaluation Framework}, \textbf{6.2 Variation by Layer}, \textbf{7 Discussions} and \textbf{8 Conclusion}.

\textit{Jeffrey} implemented the initial code for LLM inference, and ran some of the experiments. He also wrote sections \textbf{3 Threat Model}, \textbf{4.1 Design Choices}, \textbf{6.1 NaN Analysis} and part of \textbf{2 Background and Related Work}.

\textit{Arnav} implemented the benchmarking framework code using DeepEval and investigated using RMSE as a metric for evaluation. He wrote sections \textbf{5.2 Evaluation} and parts of \textbf{7 Discussions}.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:


