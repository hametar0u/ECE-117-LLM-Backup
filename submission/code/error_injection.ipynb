{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10004053,"sourceType":"datasetVersion","datasetId":6157206},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U bitsandbytes","metadata":{"_kg_hide-output":true,"papermill":{"duration":56.567766,"end_time":"2023-10-20T22:56:37.983903","exception":false,"start_time":"2023-10-20T22:55:41.416137","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U deepeval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U pydantic\n!pip install -q -U lm-format-enforcer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a dataset with the pytei files (__init__.py, pytei.py, dpytei.py) and upload the dataset named pytei-files","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append( \"/kaggle/input/pytei-files\" )\nfrom pytei import Injector","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline","metadata":{"papermill":{"duration":8.384116,"end_time":"2023-10-20T22:56:46.370893","exception":false,"start_time":"2023-10-20T22:56:37.986777","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Put your hugging face token below","metadata":{}},{"cell_type":"code","source":"token = \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T08:10:16.053737Z","iopub.execute_input":"2024-11-25T08:10:16.054169Z","iopub.status.idle":"2024-11-25T08:10:16.060078Z","shell.execute_reply.started":"2024-11-25T08:10:16.05413Z","shell.execute_reply":"2024-11-25T08:10:16.058631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 0 if torch.cuda.is_available() else -1\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deepeval.models.base_model import DeepEvalBaseLLM\nfrom typing import List\nfrom pydantic import BaseModel\nfrom lmformatenforcer import JsonSchemaParser\nfrom lmformatenforcer.integrations.transformers import (\n    build_transformers_prefix_allowed_tokens_fn,\n)\nimport transformers\nfrom transformers import pipeline\nimport json\n\nclass GPT2(DeepEvalBaseLLM):\n    def __init__(\n        self,\n        model,\n        tokenizer\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def load_model(self):\n        return self.model\n\n    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n        model = self.load_model()\n        pipeline = transformers.pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=self.tokenizer,\n            use_cache=True,\n            device_map=\"auto\",\n            max_new_tokens=100,\n            do_sample=True,\n            top_k=5,\n            num_return_sequences=1,\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        # Create parser required for JSON confinement using lmformatenforcer\n        parser = JsonSchemaParser(schema.schema())\n        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n            pipeline.tokenizer, parser\n        )\n\n        # Output and load valid JSON\n        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n        output = output_dict[0][\"generated_text\"][len(prompt) :]\n        json_result = json.loads(output)\n\n        # Return valid JSON object according to the schema DeepEval supplied\n        return schema(**json_result)\n\n    async def a_generate(self, prompt: str, schema) -> BaseModel:\n        return self.generate(prompt, schema)\n\n    # This is optional.\n    def batch_generate(self, promtps: List[str]) -> List[str]:\n        model = self.load_model()\n        device = \"cuda\" # the device to load the model onto\n\n        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n        model.to(device)\n\n        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n        return self.tokenizer.batch_decode(generated_ids)\n\n    def get_model_name(self):\n        return \"GPT2\"","metadata":{"papermill":{"duration":0.012477,"end_time":"2023-10-20T22:59:33.492407","exception":false,"start_time":"2023-10-20T22:59:33.47993","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom torch import nn\ndef print_named_params(model: nn.Module) -> None:\n    for name, param in model.named_parameters():\n        print(f\"{name}: {param.shape}\")\ndef output_targets(model: nn.Module, file: str, regex: str) -> None:\n    with open(f'{file}', 'w') as f:\n        for name, param in model.named_parameters():\n            if (re.match(regex, name)):\n                f.write(f\"{name}\\n\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You will get an error 1-2 times when running this, whenever this happens, go to \"Run\" at the top and restart the kernel (NOT the session)","metadata":{}},{"cell_type":"markdown","source":"There is some load up time for the first iteration to download the model, but everything after should be fast","metadata":{}},{"cell_type":"markdown","source":"The code will throw an error everytime layer switches so you need to be at the computer to switch layers. Each computation for a fixed layer and probability should take about 15 minutes. Whenever you restart the session you need to save your computer or the files to your computer. It would be best if you could save the output files to your local computer and just send them. They are named \"mistral_output_{x}_{???}\".","metadata":{}},{"cell_type":"code","source":"from deepeval.benchmarks import MMLU\nfrom deepeval.benchmarks.tasks import MMLUTask\nimport time\nimport gc    \n\nprobabilities = [9, 8, 7, 6, 5, 4, 3, 2, 1]\nlayers = [[str(i+j) for j in range(0, 4)] for i in range(0, 32, 4)]\nlayers = layers[1:]\nfor layer in layers:\n    model_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            #quantization_config=bnb_config,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            token=token,\n        )\n    \n    for prob in probabilities:\n        reg = '|'.join([f\"model\\.layers\\.{i}\\..*\" for i in layer])\n        target_file = f\"mistral_target_{prob}_{'_'.join(layer)}\"\n        output_file = f\"mistral_output_{prob}_{'_'.join(layer)}\"\n        output_targets(model, target_file, reg)\n\n\n    \n        benchmark = MMLU(\n            tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n            n_shots=1\n        )\n        injector = Injector(target_file, p = pow(10, -prob), device = device, dtype = torch.float16, verbose = True)\n        injector.inject_values(model)\n        mistral = GPT2(model, tokenizer)\n        benchmark.evaluate(model=mistral)\n        \n        with open(output_file, 'a') as f:\n            f.write(f'{benchmark.overall_score}\\n')","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}